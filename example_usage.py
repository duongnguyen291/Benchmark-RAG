#!/usr/bin/env python3
"""
Example usage of the Document Extraction Benchmark Framework.
This script demonstrates how to use the framework programmatically.
"""

import os
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.models import BenchmarkConfig
from src.benchmark_runner import BenchmarkRunner
from src.manual_evaluation import ManualEvaluationInterface


def example_basic_benchmark():
    """Example of running a basic benchmark."""
    print("üîç Example: Basic Benchmark")
    print("=" * 50)
    
    # Create benchmark configuration
    config = BenchmarkConfig(
        test_files_dir="./test_files",  # Directory with DOCX and PDF files
        output_dir="./results/basic_benchmark",
        repositories_to_test=["marker", "unstructured"],  # Test specific repositories
        enable_manual_evaluation=True,
        save_intermediate_results=True,
        parallel_processing=False,
        max_workers=2
    )
    
    # Run benchmark
    runner = BenchmarkRunner(config)
    summary = runner.run_benchmark()
    
    # Print results
    print(f"Benchmark completed in {summary.total_processing_time:.2f} seconds")
    print(f"Best repository: {summary.get_best_repository()}")
    print(f"Fastest repository: {summary.get_fastest_repository()}")
    
    return summary


def example_custom_evaluation():
    """Example of custom evaluation setup."""
    print("\nüîç Example: Custom Evaluation")
    print("=" * 50)
    
    # Create configuration with all repositories
    config = BenchmarkConfig(
        test_files_dir="./test_files",
        output_dir="./results/custom_evaluation",
        repositories_to_test=None,  # Test all repositories
        enable_manual_evaluation=True,
        parallel_processing=True,
        max_workers=4
    )
    
    # Run benchmark
    runner = BenchmarkRunner(config)
    summary = runner.run_benchmark()
    
    # Get repository ranking
    ranking = runner.get_repository_ranking()
    print("\nRepository Ranking (by overall score):")
    for i, (repo_id, score) in enumerate(ranking, 1):
        print(f"{i}. {repo_id}: {score:.3f}")
    
    return summary


def example_manual_evaluation():
    """Example of starting manual evaluation interface."""
    print("\nüîç Example: Manual Evaluation Interface")
    print("=" * 50)
    
    # Path to evaluation data (generated by benchmark)
    evaluation_data_path = "./results/basic_benchmark/manual_evaluation_data.json"
    
    if not os.path.exists(evaluation_data_path):
        print(f"Evaluation data not found: {evaluation_data_path}")
        print("Please run a benchmark first with --enable-manual-eval")
        return
    
    # Start manual evaluation interface
    interface = ManualEvaluationInterface(evaluation_data_path)
    
    print("Starting manual evaluation server...")
    print("The web interface will open automatically in your browser.")
    print("Press Ctrl+C to stop the server.")
    
    try:
        interface.start_server(host='localhost', port=5000, auto_open=True)
    except KeyboardInterrupt:
        print("\nManual evaluation stopped.")


def example_ground_truth_generation():
    """Example of generating ground truth only."""
    print("\nüîç Example: Ground Truth Generation")
    print("=" * 50)
    
    from src.ground_truth import GroundTruthGenerator
    
    # Initialize ground truth generator
    gt_generator = GroundTruthGenerator("./results/ground_truth")
    
    # Generate ground truth for DOCX files
    docx_files = [
        "./test_files/document1.docx",
        "./test_files/document2.docx",
        # Add more DOCX files as needed
    ]
    
    print(f"Generating ground truth for {len(docx_files)} files...")
    
    for docx_file in docx_files:
        if os.path.exists(docx_file):
            try:
                output_path, processing_time = gt_generator.generate_ground_truth(docx_file)
                print(f"‚úì Generated: {output_path} (took {processing_time.total_time:.2f}s)")
            except Exception as e:
                print(f"‚úó Failed: {docx_file} - {e}")
        else:
            print(f"‚úó File not found: {docx_file}")


def example_repository_management():
    """Example of repository management."""
    print("\nüîç Example: Repository Management")
    print("=" * 50)
    
    from src.repository_manager import RepositoryManager
    
    # Initialize repository manager
    repo_manager = RepositoryManager("./repositories")
    
    # List available repositories
    available_repos = repo_manager.list_available_repositories()
    print(f"Available repositories: {available_repos}")
    
    # Setup specific repository
    repo_id = "marker"
    print(f"\nSetting up repository: {repo_id}")
    
    success = repo_manager.setup_repository(repo_id)
    if success:
        print(f"‚úì Repository {repo_id} setup successfully")
        
        # Get repository info
        info = repo_manager.get_repository_info(repo_id)
        print(f"Repository info: {info}")
    else:
        print(f"‚úó Failed to setup repository {repo_id}")


def example_evaluation_metrics():
    """Example of using evaluation metrics directly."""
    print("\nüîç Example: Evaluation Metrics")
    print("=" * 50)
    
    from src.evaluator import DocumentEvaluator
    from src.models import ExtractionResult, ProcessingTime
    
    # Initialize evaluator
    evaluator = DocumentEvaluator()
    
    # Example extraction result (you would get this from actual extraction)
    extraction_result = ExtractionResult(
        file_path="./test_files/document1.pdf",
        repository_name="marker",
        output_path="./results/extractions/document1_marker.md",
        processing_time=ProcessingTime(total_time=1.5, extraction_time=1.5),
        success=True,
        output_content="# Example Document\n\nThis is extracted content."
    )
    
    # Ground truth path
    ground_truth_path = "./results/ground_truth/document1_ground_truth.md"
    
    if os.path.exists(ground_truth_path):
        # Evaluate extraction
        evaluation_result = evaluator.evaluate_extraction(
            extraction_result, ground_truth_path
        )
        
        print(f"Overall score: {evaluation_result.overall_score:.3f}")
        print(f"Content similarity (BERT): {evaluation_result.content_similarity.bert_score:.3f}")
        print(f"Structure accuracy: {evaluation_result.structure_accuracy.header_accuracy:.3f}")
        print(f"Formatting preservation: {evaluation_result.formatting_preservation.bold_accuracy:.3f}")
        print(f"Table quality: {evaluation_result.table_quality.content_accuracy:.3f}")
    else:
        print(f"Ground truth file not found: {ground_truth_path}")


def main():
    """Run all examples."""
    print("üöÄ Document Extraction Benchmark Framework - Examples")
    print("=" * 60)
    
    # Check if test files directory exists
    test_files_dir = "./test_files"
    if not os.path.exists(test_files_dir):
        print(f"‚ö†Ô∏è  Test files directory not found: {test_files_dir}")
        print("Please create the directory and add some DOCX and PDF files for testing.")
        print("Example structure:")
        print("  test_files/")
        print("    document1.docx")
        print("    document1.pdf")
        print("    document2.docx")
        print("    document2.pdf")
        return
    
    # Run examples
    try:
        # Basic benchmark
        example_basic_benchmark()
        
        # Custom evaluation
        example_custom_evaluation()
        
        # Repository management
        example_repository_management()
        
        # Ground truth generation
        example_ground_truth_generation()
        
        # Evaluation metrics
        example_evaluation_metrics()
        
        # Manual evaluation (commented out as it starts a server)
        # example_manual_evaluation()
        
        print("\n‚úÖ All examples completed successfully!")
        print("\nTo run manual evaluation, use:")
        print("  python main.py manual-eval ./results/basic_benchmark/manual_evaluation_data.json")
        
    except Exception as e:
        print(f"\n‚ùå Example failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
